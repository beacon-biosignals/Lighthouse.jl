var documenterSearchIndex = {"docs":
[{"location":"plotting/#Confusion-matrices","page":"Plotting","title":"Confusion matrices","text":"","category":"section"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"Lighthouse.plot_confusion_matrix","category":"page"},{"location":"plotting/#Lighthouse.plot_confusion_matrix","page":"Plotting","title":"Lighthouse.plot_confusion_matrix","text":"plot_confusion_matrix!(subfig::FigurePosition, args...; kw...)\n\nplot_confusion_matrix(confusion::AbstractMatrix{<: Number}, class_labels::AbstractVector{String}, normalize_by::Symbol;\n                      resolution=(800,600),\n                      annotation_text_size=20)\n\nLighthouse plots confusion matrices, which are simple tables showing the empirical distribution of predicted class (the rows) versus the elected class (the columns). These come in two variants:\n\nrow-normalized: this means each row has been normalized to sum to 1. Thus, the row-normalized confusion matrix shows the empirical distribution of elected classes for a given predicted class. E.g. the first row of the row-normalized confusion matrix shows the empirical probabilities of the elected classes for a sample which was predicted to be in the first class.\ncolumn-normalized: this means each column has been normalized to sum to 1. Thus, the column-normalized confusion matrix shows the empirical distribution of predicted classes for a given elected class. E.g. the first column of the column-normalized confusion matrix shows the empirical probabilities of the predicted classes for a sample which was elected to be in the first class.\n\nfig, ax, p = plot_confusion_matrix(rand(2, 2), [\"1\", \"2\"], :Row)\nfig = Figure()\nax = plot_confusion_matrix!(fig[1, 1], rand(2, 2), [\"1\", \"2\"], :Column)\n\n\n\n\n\n","category":"function"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"using Lighthouse\nusing CairoMakie\nCairoMakie.activate!(type = \"png\")\nusing StableRNGs\nconst RNG = StableRNG(22)\nstable_rand(args...) = rand(RNG, args...)\nstable_randn(args...) = randn(RNG, args...)","category":"page"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"using Lighthouse: plot_confusion_matrix, plot_confusion_matrix!\n\nclasses = [\"red\", \"orange\", \"yellow\", \"green\"]\nground_truth =     [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4]\npredicted_labels = [1, 1, 1, 1, 2, 2, 4, 4, 4, 4, 4, 3]\nconfusion = Lighthouse.confusion_matrix(length(classes), zip(predicted_labels, ground_truth))\n\nfig, ax, p = plot_confusion_matrix(confusion, classes, :Row)","category":"page"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"fig = Figure(resolution=(800, 400))\nplot_confusion_matrix!(fig[1, 1], confusion, classes, :Row, annotation_text_size=14)\nplot_confusion_matrix!(fig[1, 2], confusion, classes, :Column, annotation_text_size=14)\nfig","category":"page"},{"location":"plotting/#Theming","page":"Plotting","title":"Theming","text":"","category":"section"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"All plots are globally themeable, by setting their camelcase(functionname) to a theme. Usually, there are a few sub categories, for e.g. axis, text and subplots.","category":"page"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"warning: Warning\nMake sure, that you spell names correctly and fully construct the named tuples in the calls. E.g. (color=:red) is not a named tuple - it needs to be (color=:red,). Misspelled names and badly constructed named tuples are not easy to error on, since those theming attributes are global, and may be valid for other plots.","category":"page"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"with_theme(\n        ConfusionMatrix = (\n            Text = (\n                color=:yellow,\n            ),\n            Heatmap = (\n                colormap=:greens,\n            ),\n            Axis = (\n                backgroundcolor=:black,\n                xticklabelrotation=0.0,\n            )\n        )\n    ) do\n    plot_confusion_matrix(confusion, classes, :Row)\nend","category":"page"},{"location":"plotting/#Reliability-calibration-curves","page":"Plotting","title":"Reliability calibration curves","text":"","category":"section"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"Lighthouse.plot_reliability_calibration_curves","category":"page"},{"location":"plotting/#Lighthouse.plot_reliability_calibration_curves","page":"Plotting","title":"Lighthouse.plot_reliability_calibration_curves","text":"plot_reliability_calibration_curves!(fig::SubFigure, args...; kw...)\n\nplot_reliability_calibration_curves(per_class_reliability_calibration_curves::SeriesCurves,\n                                    per_class_reliability_calibration_scores::NumberVector,\n                                    class_labels::AbstractVector{String};\n                                    legend=:rb, resolution=(800, 600))\n\n\n\n\n\n","category":"function"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"using Lighthouse: plot_reliability_calibration_curves\nclasses = [\"class $i\" for i in 1:5]\ncurves = [(LinRange(0, 1, 10), range(0, stop=i/2, length=10) .+ (stable_randn(10) .* 0.1)) for i in -1:3]\n\nplot_reliability_calibration_curves(\n    curves,\n    stable_rand(5),\n    classes\n)","category":"page"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"Note that all curve plot types accepts these types:","category":"page"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"Lighthouse.XYVector\nLighthouse.SeriesCurves","category":"page"},{"location":"plotting/#Lighthouse.XYVector","page":"Plotting","title":"Lighthouse.XYVector","text":"Tuple{<:NumberVector, <: NumberVector}\n\nTuple of X, Y coordinates\n\n\n\n\n\n","category":"type"},{"location":"plotting/#Lighthouse.SeriesCurves","page":"Plotting","title":"Lighthouse.SeriesCurves","text":"Union{XYVector, AbstractVector{<: XYVector}}\n\nA series of XYVectors, or a single xyvector.\n\n\n\n\n\n","category":"type"},{"location":"plotting/#Theming-2","page":"Plotting","title":"Theming","text":"","category":"section"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"All generic series and axis attributes can be themed via SeriesPlot.Series / SeriesPlot.Axis. You can have a look at the series doc to get an idea about the applicable attributes. To style specifics of a subplot inside the curve plot, e.g. the ideal lineplot, one can use the camel case function name (without plot_) and pass those attributes there. So e.g the ideal curve inside the reliability curve can be themed like this:","category":"page"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"# The axis is getting created in the seriesplot,\n# to always have these kind of probabilistic series have the same axis\nseries_theme = (\n    Axis = (\n        backgroundcolor = (:gray, 0.1),\n        bottomspinevisible = false,\n        leftspinevisible = false,\n        topspinevisible = false,\n        rightspinevisible = false,\n    ),\n    Series = (\n        color=:darktest,\n        marker=:circle\n    )\n)\nwith_theme(\n        ReliabilityCalibrationCurves = (\n            Ideal = (\n                color=:red, linewidth=3\n            ),\n        ),\n        SeriesPlot = series_theme\n    ) do\n    plot_reliability_calibration_curves(\n        curves,\n        stable_rand(5),\n        classes\n    )\nend","category":"page"},{"location":"plotting/#Binary-Discrimination-Calibration-Curves","page":"Plotting","title":"Binary Discrimination Calibration Curves","text":"","category":"section"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"Lighthouse.plot_binary_discrimination_calibration_curves","category":"page"},{"location":"plotting/#Lighthouse.plot_binary_discrimination_calibration_curves","page":"Plotting","title":"Lighthouse.plot_binary_discrimination_calibration_curves","text":"plot_binary_discrimination_calibration_curves!(fig::SubFigure, args...; kw...)\n\nplot_binary_discrimination_calibration_curves!(calibration_curve::SeriesCurves, calibration_score,\n                                               per_expert_calibration_curves::SeriesCurves,\n                                               per_expert_calibration_scores, optimal_threshold,\n                                               discrimination_class::AbstractString;\n                                               marker=:rect, markersize=5, linewidth=2)\n\n\n\n\n\n","category":"function"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"using Lighthouse: plot_binary_discrimination_calibration_curves\n\nLighthouse.plot_binary_discrimination_calibration_curves(\n    curves[3],\n    stable_rand(5),\n    curves[[1, 2, 4, 5]],\n    nothing, nothing,\n    \"\",\n)","category":"page"},{"location":"plotting/#PR-curves","page":"Plotting","title":"PR curves","text":"","category":"section"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"Lighthouse.plot_pr_curves","category":"page"},{"location":"plotting/#Lighthouse.plot_pr_curves","page":"Plotting","title":"Lighthouse.plot_pr_curves","text":"plot_pr_curves!(subfig::FigurePosition, args...; kw...)\n\nplot_pr_curves(per_class_pr_curves::SeriesCurves,\n            class_labels::AbstractVector{<: String};\n            resolution=(800, 600),\n            legend=:lt, title=\"PR curves\",\n            xlabel=\"True positive rate\", ylabel=\"Precision\",\n            linewidth=2, scatter=NamedTuple(), color=:darktest)\n\nscatter::Union{Nothing, NamedTuple}: can be set to a named tuples of attributes that are forwarded to the scatter call (e.g. markersize). If nothing, no scatter is added.\n\n\n\n\n\n","category":"function"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"using Lighthouse: plot_pr_curves\nplot_pr_curves(\n    curves,\n    classes\n)","category":"page"},{"location":"plotting/#Theming-3","page":"Plotting","title":"Theming","text":"","category":"section"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"# The plots with only a series don't have a special keyword\nwith_theme(SeriesPlot = series_theme) do\n    plot_pr_curves(\n        curves,\n        classes\n    )\nend","category":"page"},{"location":"plotting/#ROC-curves","page":"Plotting","title":"ROC curves","text":"","category":"section"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"Lighthouse.plot_roc_curves","category":"page"},{"location":"plotting/#Lighthouse.plot_roc_curves","page":"Plotting","title":"Lighthouse.plot_roc_curves","text":"plot_roc_curves!(subfig::FigurePosition, args...; kw...)\n\nplot_roc_curves(per_class_roc_curves::SeriesCurves,\n                per_class_roc_aucs::NumberVector,\n                class_labels::AbstractVector{<: String};\n                resolution=(800, 600),\n                legend=:lt,\n                title=\"ROC curves\",\n                xlabel=\"False positive rate\",\n                ylabel=\"True positive rate\",\n                linewidth=2, scatter=NamedTuple(), color=:darktest)\n\nscatter::Union{Nothing, NamedTuple}: can be set to a named tuples of attributes that are forwarded to the scatter call (e.g. markersize). If nothing, no scatter is added.\n\n\n\n\n\n","category":"function"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"using Lighthouse: plot_roc_curves\n\nplot_roc_curves(\n    curves,\n    stable_rand(5),\n    classes,\n    legend=:lt)","category":"page"},{"location":"plotting/#Theming-4","page":"Plotting","title":"Theming","text":"","category":"section"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"# The plots with only a series don't have a special keyword\nwith_theme(SeriesPlot = series_theme) do\n    plot_roc_curves(\n        curves,\n        stable_rand(5),\n        classes,\n        legend=:lt)\nend","category":"page"},{"location":"plotting/#Kappas-(per-expert-agreement)","page":"Plotting","title":"Kappas (per expert agreement)","text":"","category":"section"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"Lighthouse.plot_kappas","category":"page"},{"location":"plotting/#Lighthouse.plot_kappas","page":"Plotting","title":"Lighthouse.plot_kappas","text":"plot_kappas!(subfig::FigurePosition, args...; kw...)\n\nplot_kappas(per_class_kappas::NumberVector,\n            class_labels::AbstractVector{String},\n            per_class_IRA_kappas=nothing;\n            resolution=(800, 600),\n            annotation_text_size=20)\n\n\n\n\n\n","category":"function"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"using Lighthouse: plot_kappas\nplot_kappas(stable_rand(5), classes)","category":"page"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"using Lighthouse: plot_kappas\nplot_kappas(stable_rand(5), classes, stable_rand(5))","category":"page"},{"location":"plotting/#Theming-5","page":"Plotting","title":"Theming","text":"","category":"section"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"with_theme(\n        Kappas = (\n            Axis = (\n                xticklabelvisible=false,\n                xticksvisible=false,\n                leftspinevisible = false,\n                rightspinevisible = false,\n                bottomspinevisible = false,\n                topspinevisible = false,\n            ),\n            Text = (\n                color = :blue,\n            ),\n            BarPlot = (color=[:black, :green],)\n        )) do\n    plot_kappas((1:5) ./ 5 .- 0.1, classes, (1:5) ./ 5)\nend","category":"page"},{"location":"plotting/#Evaluation-metrics-plot","page":"Plotting","title":"Evaluation metrics plot","text":"","category":"section"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"Lighthouse.evaluation_metrics_plot","category":"page"},{"location":"plotting/#Lighthouse.evaluation_metrics_plot","page":"Plotting","title":"Lighthouse.evaluation_metrics_plot","text":"evaluation_metrics_plot(data::Dict; resolution=(1000, 1000), textsize=12)\n\nPlots all evaluation metrics Lighthouse has to offer.\n\n\n\n\n\nevaluation_metrics_plot(predicted_hard_labels::AbstractVector,\n                        predicted_soft_labels::AbstractMatrix,\n                        elected_hard_labels::AbstractVector,\n                        classes,\n                        thresholds=0.0:0.01:1.0;\n                        votes::Union{Nothing,AbstractMatrix}=nothing,\n                        strata::Union{Nothing,AbstractVector{Set{T}} where T}=nothing,\n                        optimal_threshold_class::Union{Nothing,Integer}=nothing)\n\nReturn a plot and dictionary containing a battery of classifier performance metrics that each compare predicted_soft_labels and/or predicted_hard_labels agaist elected_hard_labels.\n\nSee evaluation_metrics for a description of the arguments.\n\nThis method is deprecated in favor of calling evaluation_metrics and evaluation_metrics_plot separately.\n\n\n\n\n\n","category":"function"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"using Lighthouse: evaluation_metrics_plot\ndata = Dict{String, Any}()\ndata[\"confusion_matrix\"] = stable_rand(5, 5)\ndata[\"class_labels\"] = classes\n\ndata[\"per_class_kappas\"] = stable_rand(5)\ndata[\"multiclass_kappa\"] = stable_rand()\ndata[\"per_class_IRA_kappas\"] = stable_rand(5)\ndata[\"multiclass_IRA_kappas\"] = stable_rand()\n\ndata[\"per_class_pr_curves\"] = curves\ndata[\"per_class_roc_curves\"] = curves\ndata[\"per_class_roc_aucs\"] = stable_rand(5)\n\ndata[\"per_class_reliability_calibration_curves\"] = curves\ndata[\"per_class_reliability_calibration_scores\"] = stable_rand(5)\n\nevaluation_metrics_plot(data)","category":"page"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"Optionally, one can also add a binary discrimination calibration curve plot:","category":"page"},{"location":"plotting/","page":"Plotting","title":"Plotting","text":"\ndata[\"discrimination_calibration_curve\"] = (LinRange(0, 1, 10), LinRange(0,1, 10) .+ 0.1randn(10))\ndata[\"per_expert_discrimination_calibration_curves\"] = curves\n\n# These are currently not used in plotting, but are still passed to `plot_binary_discrimination_calibration_curves`!\ndata[\"discrimination_calibration_score\"] = nothing\ndata[\"optimal_threshold_class\"] = 1\ndata[\"per_expert_discrimination_calibration_scores\"] = nothing\ndata[\"optimal_threshold\"] = nothing\n\nevaluation_metrics_plot(data)","category":"page"},{"location":"#API-Documentation","page":"API Documentation","title":"API Documentation","text":"","category":"section"},{"location":"","page":"API Documentation","title":"API Documentation","text":"CurrentModule = Lighthouse","category":"page"},{"location":"#The-AbstractClassifier-Interface","page":"API Documentation","title":"The AbstractClassifier Interface","text":"","category":"section"},{"location":"","page":"API Documentation","title":"API Documentation","text":"AbstractClassifier\nLighthouse.classes\nLighthouse.train!\nLighthouse.loss_and_prediction\nLighthouse.onehot\nLighthouse.onecold\nLighthouse.is_early_stopping_exception","category":"page"},{"location":"#Lighthouse.AbstractClassifier","page":"API Documentation","title":"Lighthouse.AbstractClassifier","text":"AbstractClassifier\n\nAn abstract type whose subtypes C<:AbstractClassifier must implement:\n\nLighthouse.classes\nLighthouse.train!\nLighthouse.loss_and_prediction\n\nSubtypes may additionally overload default implementations for:\n\nLighthouse.onehot\nLighthouse.onecold\nLighthouse.is_early_stopping_exception\n\nThe AbstractClassifier interface is built upon the expectation that any multiclass label will be represented in one of two standardized forms:\n\n\"soft label\": a probability distribution vector where the ith element is the probability assigned to the ith class in classes(classifier).\n\"hard label\": the interger index of a corresponding class in classes(classifier).\n\nInternally, Lighthouse converts hard labels to soft labels via onehot and soft labels to hard labels via onecold.\n\nSee also: learn!\n\n\n\n\n\n","category":"type"},{"location":"#Lighthouse.classes","page":"API Documentation","title":"Lighthouse.classes","text":"Lighthouse.classes(classifier::AbstractClassifier)\n\nReturn a Vector or Tuple of class values for classifier.\n\nThis method must be implemented for each AbstractClassifier subtype.\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse.train!","page":"API Documentation","title":"Lighthouse.train!","text":"Lighthouse.train!(classifier::AbstractClassifier, batches, logger::LearnLogger)\n\nTrain classifier on the iterable batches for a single epoch. This function is called once per epoch by learn!.\n\nThis method must be implemented for each AbstractClassifier subtype. Implementers should ensure that the training loss is properly logged to logger by calling Lighthouse.log_value!(logger, \"train/loss_per_batch\", batch_loss) for each batch in batches.\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse.loss_and_prediction","page":"API Documentation","title":"Lighthouse.loss_and_prediction","text":"Lighthouse.loss_and_prediction(classifier::AbstractClassifier,\n                               input_batch::AbstractArray,\n                               args...)\n\nReturn (loss, soft_label_batch) given input_batch and any additional args provided by the caller; loss is a scalar, which soft_label_batch is a matrix with length(classes(classifier)) rows and size(input_batch).\n\nSpecifically, the ith column of soft_label_batch is classifier's soft label prediction for the ith sample in input_batch.\n\nThis method must be implemented for each AbstractClassifier subtype.\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse.onehot","page":"API Documentation","title":"Lighthouse.onehot","text":"Lighthouse.onehot(classifier::AbstractClassifier, hard_label)\n\nReturn the one-hot encoded probability distribution vector corresponding to the given hard_label. hard_label must be an integer index in the range 1:length(classes(classifier)).\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse.onecold","page":"API Documentation","title":"Lighthouse.onecold","text":"Lighthouse.onecold(classifier::AbstractClassifier, soft_label)\n\nReturn the hard label (integer index in the range 1:length(classes(classifier))) corresponding to the given soft_label (one-hot encoded probability distribution vector).\n\nBy default, this function returns argmax(soft_label).\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse.is_early_stopping_exception","page":"API Documentation","title":"Lighthouse.is_early_stopping_exception","text":"Lighthouse.is_early_stopping_exception(classifier::AbstractClassifier, exception)\n\nReturn true if exception should be considered an \"early-stopping exception\" (e.g. Flux.Optimise.StopException), rather than rethrown from learn!.\n\nThis function returns false by default, but can be overloaded by subtypes of AbstractClassifier that employ exceptions as early-stopping mechanisms.\n\n\n\n\n\n","category":"function"},{"location":"#The-learn!-Interface","page":"API Documentation","title":"The learn! Interface","text":"","category":"section"},{"location":"","page":"API Documentation","title":"API Documentation","text":"LearnLogger\nlearn!\nupon\nevaluate!\npredict!\nLighthouse.forward_logs\nLighthouse._calculate_ea_kappas\nLighthouse._calculate_ira_kappas\nLighthouse._calculate_spearman_correlation","category":"page"},{"location":"#Lighthouse.LearnLogger","page":"API Documentation","title":"Lighthouse.LearnLogger","text":"LearnLogger\n\nA struct that wraps a TensorBoardLogger.TBLogger in order to enforce the following:\n\nall values logged to Tensorboard should be accessible to the post_epoch_callback argument to learn!\nall values that are cached during learn! should be logged to Tensorboard\n\nTo access values logged to a LearnLogger instance, inspect the instance's logged field.\n\n\n\n\n\n","category":"type"},{"location":"#Lighthouse.learn!","page":"API Documentation","title":"Lighthouse.learn!","text":"learn!(model::AbstractClassifier, logger,\n       get_train_batches, get_test_batches, votes,\n       elected=majority.(eachrow(votes), (1:length(classes(model)),));\n       epoch_limit=100, post_epoch_callback=(_ -> nothing),\n       optimal_threshold_class::Union{Nothing,Integer}=nothing,\n       test_set_logger_prefix=\"test_set\")\n\nReturn model after optimizing its parameters across multiple epochs of training and test, logging Lighthouse's standardized suite of classifier performance metrics to logger throughout the optimization process.\n\nThe following phases are executed at each epoch (note: in the below lists of logged values, $resource takes the values of the field names of Lighthouse.ResourceInfo):\n\nTrain model by calling train!(model, get_train_batches(), logger). The following quantities are logged to logger during this phase:\ntrain/loss_per_batch\nany additional quantities logged by the relevant model/framework-specific implementation of train!.\nCompute model's predictions on test set provided by get_test_batches() (see below for details). The following quantities are logged to logger during this phase:\n<test_set_logger_prefix>_prediction/loss_per_batch\n<test_set_logger_prefix>_prediction/mean_loss_per_epoch\n<test_set_logger_prefix>_prediction/$resource_per_batch\nCompute a battery of metrics to evaluate model's performance on the test set based on the test set prediction phase. The following quantities are logged to logger during this phase:\n<test_set_logger_prefix>_evaluation/metrics_per_epoch\n<test_set_logger_prefix>_evaluation/$resource_per_epoch\nCall post_epoch_callback(current_epoch).\n\nWhere...\n\nget_train_batches is a zero-argument function that returns an iterable of training set batches. Internally, learn! uses this function when it calls train!(model, get_train_batches(), logger).\nget_test_batches is a zero-argument function that returns an iterable of test set batches used during the current epoch's test phase. Each element of the iterable takes the form (batch, votes_locations). Internally, batch is passed to loss_and_prediction as loss_and_prediction(model, batch...), and votes_locations[i] is expected to yield the row index of votes that corresponds to the ith sample in batch.\nvotes is a matrix of hard labels whose columns correspond to voters and whose rows correspond to the samples in the test set that have been voted on. If votes[sample, voter] is not a valid hard label for model, then voter will simply be considered to have not assigned a hard label to sample.\nelected is a vector of hard labels where the ith element is the hard label elected as \"ground truth\" out of votes[i, :].\noptimal_threshold_class is the class index (1 or 2) for which to calculate an optimal threshold for converting predicted_soft_labels to predicted_hard_labels. This is only a valid parameter when length(classes) == 2. If optimal_threshold_class is present, test set evaluation will be based on predicted hard labels calculated with this threshold; if optimal_threshold_class is nothing, predicted hard labels will be calculated via onecold(classifier, soft_label).\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse.upon","page":"API Documentation","title":"Lighthouse.upon","text":"upon(logged::Dict{String,Any}, field::AbstractString; condition, initial)\n\nReturn a closure that can be called to check the most recent state of logger.logged[field] and trigger a caller-provided function when condition(recent_state, previously_chosen_state) is true.\n\nFor example:\n\nupon_loss_decrease = upon(logger, \"test_set_prediction/mean_loss_per_epoch\";\n                          condition=<, initial=Inf)\n\nsave_upon_loss_decrease = _ -> begin\n    upon_loss_decrease(new_lowest_loss -> save_my_model(model, new_lowest_loss),\n                       consecutive_failures -> consecutive_failures > 10 && Flux.stop())\nend\n\nlearn!(model, logger, get_train_batches, get_test_batches, votes;\n       post_epoch_callback=save_upon_loss_decrease)\n\nSpecifically, the form of the returned closure is f(on_true, on_false) where on_true(state) is called if condition(state, previously_chosen_state) is true. Otherwise, on_false(consecutive_falses) is called where consecutive_falses is the number of condition calls that have returned false since the last condition call returned true.\n\nNote that the returned closure is a no-op if logger.logged[field] has not been updated since the most recent call.\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse.evaluate!","page":"API Documentation","title":"Lighthouse.evaluate!","text":"evaluate!(predicted_hard_labels::AbstractVector,\n          predicted_soft_labels::AbstractMatrix,\n          elected_hard_labels::AbstractVector,\n          classes, logger::LearnLogger;\n          logger_prefix, logger_suffix,\n          votes::Union{Nothing,AbstractMatrix}=nothing,\n          thresholds=0.0:0.01:1.0,\n          optimal_threshold_class::Union{Nothing,Integer}=nothing)\n\nReturn nothing after computing and logging a battery of classifier performance metrics that each compare predicted_soft_labels and/or predicted_hard_labels agaist elected_hard_labels.\n\nThe following quantities are logged to logger:     - <logger_prefix>/metrics<logger_suffix>     - <logger_prefix>/$resource<logger_suffix>\n\nWhere...\n\npredicted_soft_labels is a matrix of soft labels whose columns correspond to classes and whose rows correspond to samples in the evaluation set.\npredicted_hard_labels is a vector of hard labels where the ith element is the hard label predicted by the model for sample i in the evaulation set.\nelected_hard_labels is a vector of hard labels where the ith element is the hard label elected as \"ground truth\" for sample i in the evaulation set.\nthresholds are the range of thresholds used by metrics (e.g. PR curves) that are calculated on the predicted_soft_labels for a range of thresholds.\nvotes is a matrix of hard labels whose columns correspond to voters and whose rows correspond to the samples in the test set that have been voted on. If votes[sample, voter] is not a valid hard label for model, then voter will simply be considered to have not assigned a hard label to sample.\noptimal_threshold_class is the class index (1 or 2) for which to calculate an optimal threshold for converting the predicted_soft_labels to predicted_hard_labels. If present, the input predicted_hard_labels will be ignored and new predicted_hard_labels will be recalculated from the new threshold. This is only a valid parameter when length(classes) == 2\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse.predict!","page":"API Documentation","title":"Lighthouse.predict!","text":"predict!(model::AbstractClassifier,\n         predicted_soft_labels::AbstractMatrix,\n         batches, logger::LearnLogger;\n         logger_prefix::AbstractString)\n\nReturn mean_loss of all batches after using model to predict their soft labels and storing those results in predicted_soft_labels.\n\nThe following quantities are logged to logger:\n\n<logger_prefix>/loss_per_batch\n<logger_prefix>/mean_loss_per_epoch\n<logger_prefix>/$resource_per_batch\n\nWhere...\n\nmodel is a model that outputs soft labels when called on a batch of batches, model(batch).\npredicted_soft_labels is a matrix whose columns correspond to classes and whose rows correspond to samples in batches, and which is filled in with soft-label predictions.\nbatches is an iterable of batches, where each element of the iterable takes the form (batch, votes_locations). Internally, batch is passed to loss_and_prediction as loss_and_prediction(model, batch...).\n\n\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse.forward_logs","page":"API Documentation","title":"Lighthouse.forward_logs","text":"forwarding_task = forward_logs(channel, logger::LearnLogger)\n\nForwards logs with values supported by TensorBoardLogger to logger::LearnLogger:\n\nstring events of type AbstractString\nscalars of type Union{Real,Complex}\nplots that TensorBoardLogger can convert to raster images\n\nreturns the forwarding_task:::Task that does the forwarding. To cleanly stop forwarding, close(channel) and wait(forwarding_task).\n\noutbox is a Channel or RemoteChannel of Pair{String, Any} field names starting with \"plot\" forward to TensorBoardLogger.log_image\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse._calculate_ea_kappas","page":"API Documentation","title":"Lighthouse._calculate_ea_kappas","text":"_calculate_ea_kappas(predicted_hard_labels, elected_hard_labels, classes)\n\nReturn NamedTuple with keys :per_class_kappas, :multiclass_kappa containing the Cohen's Kappa per-class and over all classes, respectively. The value of output key :per_class_kappas is an Array such that item i is the Cohen's kappa calculated for class i.\n\nWhere...\n\npredicted_hard_labels is a vector of hard labels where the ith element is the hard label predicted by the model for sample i in the evaulation set.\nelected_hard_labels is a vector of hard labels where the ith element is the hard label elected as \"ground truth\" for sample i in the evaulation set.\nclass_count is the number of possible classes.\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse._calculate_ira_kappas","page":"API Documentation","title":"Lighthouse._calculate_ira_kappas","text":"_calculate_ira_kappas(votes, classes)\n\nReturn NamedTuple with keys :per_class_IRA_kappas, :multiclass_IRA_kappas containing the Cohen's Kappa for inter-rater agreement (IRA) per-class and over all classes, respectively. The value of output key :per_class_IRA_kappas is an Array such that item i is the IRA kappa calculated for class i.\n\nWhere...\n\nvotes is a matrix of hard labels whose columns correspond to voters and whose rows correspond to the samples in the test set that have been voted on. If votes[sample, voter] is not a valid hard label for model, then voter will simply be considered to have not assigned a hard label to sample.\nclasses all possible classes voted on.\n\nReturns (per_class_IRA_kappas=missing, multiclass_IRA_kappas=missing) if votes has only a single voter (i.e., a single column) or if no two voters rated the same sample. Note that vote entries of 0 are taken to mean that the voter did not rate that sample.\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse._calculate_spearman_correlation","page":"API Documentation","title":"Lighthouse._calculate_spearman_correlation","text":"_calculate_spearman_correlation(predicted_soft_labels, votes, classes)\n\nReturn NamedTuple with keys :ρ, :n, :ci_lower, and ci_upper that are the Spearman correlation constant ρ and its 95% confidence interval bounds. Only valid for binary classification problems (i.e., length(classes) == 2)\n\nWhere...\n\npredicted_soft_labels is a matrix of soft labels whose columns correspond to the two classes and whose rows correspond to the samples in the test set that have been classified. For a given sample, the two class column values must sum to 1 (i.e., softmax has been applied to the classification output).\nvotes is a matrix of hard labels whose columns correspond to voters and whose rows correspond to the samples in the test set that have been voted on. If votes[sample, voter] is not a valid hard label for model, then voter will simply be considered to have not assigned a hard label to sample. May contain a single voter (i.e., a single column).\nclasses are the two classes voted on.\n\n\n\n\n\n","category":"function"},{"location":"#Performance-Metrics","page":"API Documentation","title":"Performance Metrics","text":"","category":"section"},{"location":"","page":"API Documentation","title":"API Documentation","text":"confusion_matrix\naccuracy\nbinary_statistics\ncohens_kappa\ncalibration_curve\nEvaluationRow\nObservationRow\nLighthouse.evaluation_metrics\nLighthouse._evaluation_row_dict\nLighthouse.evaluation_metrics_row","category":"page"},{"location":"#Lighthouse.confusion_matrix","page":"API Documentation","title":"Lighthouse.confusion_matrix","text":"confusion_matrix(class_count::Integer, hard_label_pairs = ())\n\nGiven the iterable hard_label_pairs whose kth element takes the form (first_classifiers_label_for_sample_k, second_classifiers_label_for_sample_k), return the corresponding confusion matrix where matrix[i, j] is the number of samples that the first classifier labeled i and the second classifier labeled j.\n\nNote that the returned confusion matrix can be updated in-place with new labels via Lighthouse.increment_at!(matrix, more_hard_label_pairs).\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse.accuracy","page":"API Documentation","title":"Lighthouse.accuracy","text":"accuracy(confusion::AbstractMatrix)\n\nReturns the percentage of matching classifications out of total classifications, or NaN if all(iszero, confusion).\n\nNote that accuracy(confusion) is equivalent to overall percent agreement between confusion's row classifier and column classifier.\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse.binary_statistics","page":"API Documentation","title":"Lighthouse.binary_statistics","text":"binary_statistics(confusion::AbstractMatrix, class_index)\n\nTreating the rows of confusion as corresponding to predicted classifications and the columns as corresponding to true classifications, return a NamedTuple with the following fields for the given class_index:\n\npredicted_positives\npredicted_negatives\nactual_positives\nactual_negatives\ntrue_positives\ntrue_negatives\nfalse_positives\nfalse_negatives\ntrue_positive_rate\ntrue_negative_rate\nfalse_positive_rate\nfalse_negative_rate\nprecision\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse.cohens_kappa","page":"API Documentation","title":"Lighthouse.cohens_kappa","text":"cohens_kappa(class_count, hard_label_pairs)\n\nReturn (κ, p₀) where κ is Cohen's kappa and p₀ percent agreement given class_count and hard_label_pairs (these arguments take the same form as their equivalents in confusion_matrix).\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse.calibration_curve","page":"API Documentation","title":"Lighthouse.calibration_curve","text":"calibration_curve(probabilities, bitmask; bin_count=10)\n\nGiven probabilities (the predicted probabilities of the positive class) and bitmask (a vector of Bools indicating whether or not the element actually belonged to the positive class), return (bins, fractions, totals, mean_squared_error) where:\n\nbins a vector with bin_count Pairs specifying the calibration curve's probability bins\nfractions: a vector where fractions[i] is the number of values in probabilities that falls within bin[i] over the total number of values within bin[i], or NaN if the total number of values in bin[i] is zero.\ntotals: a vector where totals[i] the total number of values within bin[i].\nmean_squared_error: The mean squared error of fractions vs. an ideal calibration curve.\n\nThis method is similar to the corresponding scikit-learn method:\n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.calibration.calibration_curve.html\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse.EvaluationRow","page":"API Documentation","title":"Lighthouse.EvaluationRow","text":"const EvaluationRow = Legolas.@row(\"lighthouse.evaluation@1\",\n                               class_labels::Union{Missing,Vector{String}},\n                               confusion_matrix::Union{Missing,Array{Int64}} = vec_to_mat(confusion_matrix),\n                               discrimination_calibration_curve::Union{Missing,\n                                                                       Tuple{Vector{Float64},\n                                                                             Vector{Float64}}},\n                               discrimination_calibration_score::Union{Missing,Float64},\n                               multiclass_IRA_kappas::Union{Missing,Float64},\n                               multiclass_kappa::Union{Missing,Float64},\n                               optimal_threshold::Union{Missing,Float64},\n                               optimal_threshold_class::Union{Missing,Int64},\n                               per_class_IRA_kappas::Union{Missing,Vector{Float64}},\n                               per_class_kappas::Union{Missing,Vector{Float64}},\n                               stratified_kappas::Union{Missing,\n                                                        Vector{NamedTuple{(:per_class,\n                                                                           :multiclass,\n                                                                           :n),\n                                                                          Tuple{Vector{Float64},\n                                                                                Float64,\n                                                                                Int64}}}},\n                               per_class_pr_curves::Union{Missing,\n                                                          Vector{Tuple{Vector{Float64},\n                                                                       Vector{Float64}}}},\n                               per_class_reliability_calibration_curves::Union{Missing,\n                                                                               Vector{Tuple{Vector{Float64},\n                                                                                            Vector{Float64}}}},\n                               per_class_reliability_calibration_scores::Union{Missing,\n                                                                               Vector{Float64}},\n                               per_class_roc_aucs::Union{Missing,Vector{Float64}},\n                               per_class_roc_curves::Union{Missing,\n                                                           Vector{Tuple{Vector{Float64},\n                                                                        Vector{Float64}}}},\n                               per_expert_discrimination_calibration_curves::Union{Missing,\n                                                                                   Vector{Tuple{Vector{Float64},\n                                                                                                Vector{Float64}}}},\n                               per_expert_discrimination_calibration_scores::Union{Missing,\n                                                                                   Vector{Float64}},\n                               spearman_correlation::Union{Missing,\n                                                           NamedTuple{(:ρ, :n,\n                                                                       :ci_lower,\n                                                                       :ci_upper),\n                                                                      Tuple{Float64,\n                                                                            Int64,\n                                                                            Float64,\n                                                                            Float64}}},\n                               thresholds::Union{Missing,Vector{Float64}})\nEvaluationRow(evaluation_row_dict::Dict{String, Any}) -> EvaluationRow\n\nA type alias for Legolas.Row{typeof(Legolas.Schema(\"lighthouse.evaluation@1@1\"))} representing the output metrics computed by evaluation_metrics_row and evaluation_metrics.\n\nConstructor that takes evaluation_row_dict converts evaluation_metrics Dict of metrics results (e.g. from Lighthouse <v0.14.0) into an EvaluationRow.\n\n\n\n\n\n","category":"type"},{"location":"#Lighthouse.ObservationRow","page":"API Documentation","title":"Lighthouse.ObservationRow","text":"const ObservationRow = Legolas.@row(\"lighthouse.observation@1\",\n                                    predicted_hard_label::Int64,\n                                    predicted_soft_labels::Vector{Float32},\n                                    elected_hard_label::Int64,\n                                    votes::Union{Missing,Vector{Int64}})\n\nA type alias for Legolas.Row{typeof(Legolas.Schema(\"lighthouse.observation@1\"))} representing the per-observation input values required to compute evaluation_metrics_row.\n\n\n\n\n\n","category":"type"},{"location":"#Lighthouse.evaluation_metrics","page":"API Documentation","title":"Lighthouse.evaluation_metrics","text":"evaluation_metrics(args...; optimal_threshold_class=nothing, kwargs...)\n\nReturn evaluation_metrics_row after converting output EvaluationRow into a Dict. For argument details, see evaluation_metrics_row.\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse._evaluation_row_dict","page":"API Documentation","title":"Lighthouse._evaluation_row_dict","text":"_evaluation_row_dict(row::EvaluationRow) -> Dict{String,Any}\n\nConvert EvaluationRow into ::Dict{String, Any} results, as are output by [evaluation_metrics](@ref) (and predated use of EvaluationRow in Lighthouse <v0.14.0).\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse.evaluation_metrics_row","page":"API Documentation","title":"Lighthouse.evaluation_metrics_row","text":"evaluation_metrics_row(observation_table, classes, thresholds=0.0:0.01:1.0;\n                       strata::Union{Nothing,AbstractVector{Set{T}} where T}=nothing,\n                       optimal_threshold_class::Union{Missing,Nothing,Integer}=missing)\nevaluation_metrics_row(predicted_hard_labels::AbstractVector,\n                       predicted_soft_labels::AbstractMatrix,\n                       elected_hard_labels::AbstractVector,\n                       classes,\n                       thresholds=0.0:0.01:1.0;\n                       votes::Union{Nothing,Missing,AbstractMatrix}=nothing,\n                       strata::Union{Nothing,AbstractVector{Set{T}} where T}=nothing,\n                       optimal_threshold_class::Union{Missing,Nothing,Integer}=missing)\n\nReturns EvaluationRow containing a battery of classifier performance metrics that each compare predicted_soft_labels and/or predicted_hard_labels agaist elected_hard_labels.\n\nWhere...\n\npredicted_soft_labels is a matrix of soft labels whose columns correspond to classes and whose rows correspond to samples in the evaluation set.\npredicted_hard_labels is a vector of hard labels where the ith element is the hard label predicted by the model for sample i in the evaulation set.\nelected_hard_labels is a vector of hard labels where the ith element is the hard label elected as \"ground truth\" for sample i in the evaulation set.\nthresholds are the range of thresholds used by metrics (e.g. PR curves) that are calculated on the predicted_soft_labels for a range of thresholds.\nvotes is a matrix of hard labels whose columns correspond to voters and whose rows correspond to the samples in the test set that have been voted on. If votes[sample, voter] is not a valid hard label for model, then voter will simply be considered to have not assigned a hard label to sample.\nstrata is a vector of sets of (arbitrarily typed) groups/strata for each sample in the evaluation set, or nothing. If not nothing, per-class and multiclass kappas will also be calculated per group/stratum.\noptimal_threshold_class is the class index (1 or 2) for which to calculate an optimal threshold for converting the predicted_soft_labels to predicted_hard_labels. If present, the input predicted_hard_labels will be ignored and new predicted_hard_labels will be recalculated from the new threshold. This is only a valid parameter when length(classes) == 2\n\nAlternatively, an observation_table that consists of rows of type ObservationRow can be passed in in place of predicted_soft_labels,predicted_hard_labels,elected_hard_labels, and votes.\n\nSee also evaluation_metrics_plot.\n\n\n\n\n\n","category":"function"},{"location":"#Utilities","page":"API Documentation","title":"Utilities","text":"","category":"section"},{"location":"","page":"API Documentation","title":"API Documentation","text":"majority\nLighthouse.area_under_curve\nLighthouse.area_under_curve_unit_square\nflush(::LearnLogger)","category":"page"},{"location":"#Lighthouse.majority","page":"API Documentation","title":"Lighthouse.majority","text":"majority([rng::AbstractRNG=Random.GLOBAL_RNG], hard_labels, among::UnitRange)\n\nReturn the majority label within among out of hard_labels:\n\njulia> majority([1, 2, 1, 3, 2, 2, 3], 1:3)\n2\n\njulia> majority([1, 2, 1, 3, 2, 2, 3, 4], 3:4)\n3\n\nIn the event of a tie, a winner is randomly selected from the tied labels via rng.\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse.area_under_curve","page":"API Documentation","title":"Lighthouse.area_under_curve","text":"area_under_curve(x, y)\n\nCalculates the area under the curve specified by the x vector and y vector using the trapezoidal rule. If inputs are empty, return missing.\n\n\n\n\n\n","category":"function"},{"location":"#Lighthouse.area_under_curve_unit_square","page":"API Documentation","title":"Lighthouse.area_under_curve_unit_square","text":"area_under_curve_unit_square(x, y)\n\nCalculates the area under the curve specified by the x vector and y vector for a unit square, using the trapezoidal rule. If inputs are empty, return missing.\n\n\n\n\n\n","category":"function"},{"location":"#Base.flush-Tuple{LearnLogger}","page":"API Documentation","title":"Base.flush","text":"Base.flush(logger)\n\nPersist possibly transient logger state.\n\n\n\n\n\n","category":"method"}]
}
